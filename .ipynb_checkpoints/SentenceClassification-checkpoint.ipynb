{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Classification using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'finegrained.txt'\n",
    "\n",
    "def read_data(file):\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "    \n",
    "        sent = ['pos', 'neg', 'neu']\n",
    "\n",
    "        sentiment = []\n",
    "        review = []\n",
    "\n",
    "        for sen in f:\n",
    "            if len(sen.strip()) != 0 :\n",
    "\n",
    "                row = sen.strip().split()\n",
    "\n",
    "                if row[0] in sent:\n",
    "\n",
    "                    sentiment.append(row[0])\n",
    "                    review.append(row[1:])\n",
    "    \n",
    "    return sentiment, review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of sentence: 155\n",
      "\n",
      "sentiment: ['neg', 'neg', 'neg', 'neg', 'neg']\n",
      "\n",
      "review:\n",
      " [['The', 'book', 'is', 'disproportionally', 'focused', 'on', 'single', 'and', 'multilayer', 'feedforward', 'networks.'], ['And', 'though', 'the', 'book', 'puts', 'great', 'emphasis', 'on', 'mathematics', 'and', 'even', 'includes', 'a', 'big', 'section', 'on', 'important', 'mathematical', 'background', 'knowledge,', 'it', 'contains', 'to', 'many', 'errors', 'in', 'the', 'mathematical', 'formulas,', 'so', 'they', 'are', 'of', 'little', 'use.'], ['The', 'author', \"hasn't\", 'even', 'taken', 'the', 'trouble', 'to', 'put', 'up', 'an', 'errata', 'list.'], ['Finally,', 'for', 'the', 'beginner', 'there', 'are', 'not', 'enough', 'conceptual', 'clues', 'on', 'what', 'is', 'actually', 'going', 'on', 'and', 'it', 'is', 'hard', 'to', 'form', 'any', 'mental', 'model', 'of', 'the', 'underlying', 'processes.'], ['There', 'are', 'better', 'books.']]\n"
     ]
    }
   ],
   "source": [
    "sent, rev = read_data(fname)\n",
    "\n",
    "print('max length of sentence:', len(max(rev, key = len)))\n",
    "print('\\nsentiment:',sent[:5])\n",
    "print('\\nreview:\\n', rev[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and shortening the sentences with certain threshold and making the reviews of equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 50\n",
    "\n",
    "def normalizing_reviews(reviews):\n",
    "    norm_review = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        if len(review) < threshold:\n",
    "            norm_review.append(review + ['PAD']*(threshold - len(review)))\n",
    "        \n",
    "        elif len(review) > threshold:\n",
    "            norm_review.append(review[:threshold])\n",
    "            \n",
    "        else:\n",
    "            norm_review.append(review)\n",
    "            \n",
    "    return norm_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review length: 50\n",
      "\n",
      "reviews:\n",
      " [['The', 'book', 'is', 'disproportionally', 'focused', 'on', 'single', 'and', 'multilayer', 'feedforward', 'networks.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'], ['And', 'though', 'the', 'book', 'puts', 'great', 'emphasis', 'on', 'mathematics', 'and', 'even', 'includes', 'a', 'big', 'section', 'on', 'important', 'mathematical', 'background', 'knowledge,', 'it', 'contains', 'to', 'many', 'errors', 'in', 'the', 'mathematical', 'formulas,', 'so', 'they', 'are', 'of', 'little', 'use.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'], ['The', 'author', \"hasn't\", 'even', 'taken', 'the', 'trouble', 'to', 'put', 'up', 'an', 'errata', 'list.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'], ['Finally,', 'for', 'the', 'beginner', 'there', 'are', 'not', 'enough', 'conceptual', 'clues', 'on', 'what', 'is', 'actually', 'going', 'on', 'and', 'it', 'is', 'hard', 'to', 'form', 'any', 'mental', 'model', 'of', 'the', 'underlying', 'processes.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'], ['There', 'are', 'better', 'books.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']]\n"
     ]
    }
   ],
   "source": [
    "norm_rev = normalizing_reviews(rev)\n",
    "\n",
    "print('review length:', len(norm_rev[0]))\n",
    "print('\\nreviews:\\n', norm_rev[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "def create_dict(normalized_reviews):\n",
    "    global vocab_size\n",
    "    words = []\n",
    "    \n",
    "    for review in normalized_reviews:\n",
    "        words.extend(review)\n",
    "    print('length of words:', len(words))\n",
    "    print('words in the vocabulary: %d'%len(collections.Counter(words).most_common()))\n",
    "\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    rev_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    return dictionary, rev_dictionary, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of words: 125750\n",
      "words in the vocabulary: 11564\n",
      "dictionary ['marriage', 'policy', 'Martin.', 'expansion', 'punk', 'summarizes', 'artifact', 'diskette,', 'dimensional.', 'on,']\n",
      "reverse dictionary [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "most common words: [['UNK', -1], ('PAD', 78662), ('the', 2262), ('and', 1230), ('to', 1101)]\n",
      "len of dictionary: 10000\n"
     ]
    }
   ],
   "source": [
    "dictionary, rev_dictionary, count = create_dict(norm_rev)\n",
    "\n",
    "print('dictionary', list(dictionary)[:10])\n",
    "print('reverse dictionary', list(rev_dictionary)[:10])\n",
    "print('most common words:', count[0:5])\n",
    "print('len of dictionary:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting str to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(normalized_reviews, dictionary):\n",
    "    \n",
    "    review_int = []\n",
    "    \n",
    "    for review in normalized_reviews:\n",
    "        norm_rev_int = []\n",
    "        \n",
    "        for word in review:\n",
    "            if word in dictionary:\n",
    "                norm_rev_int.append(dictionary[word])\n",
    "            else:\n",
    "                norm_rev_int.append(dictionary['UNK'])\n",
    "        \n",
    "        review_int.append(norm_rev_int)\n",
    "    \n",
    "    return review_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review int[0]:\n",
      " [14, 69, 7, 8014, 2103, 17, 361, 3, 7278, 7111, 2745, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "review int[1]:\n",
      " [129, 321, 2, 69, 881, 71, 1453, 17, 7691, 3, 60, 1008, 5, 277, 1753, 17, 978, 2329, 956, 0, 11, 1437, 4, 99, 1838, 9, 2, 2329, 4662, 30, 33, 18, 6, 98, 887, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "rev_int = str_to_int(norm_rev, dictionary)\n",
    "\n",
    "print('review int[0]:\\n', rev_int[0])\n",
    "print('\\nreview int[1]:\\n', rev_int[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = random.sample(list(range(len(rev_int))), 500)\n",
    "\n",
    "train_rev = [rev_int[idx] for idx in range(len(rev_int)) if idx not in test_indices]\n",
    "test_rev = [rev_int[idx] for idx in test_indices]\n",
    "             \n",
    "train_sent = [sent[idx] for idx in range(len(rev_int)) if idx not in test_indices]\n",
    "test_sent = [sent[idx] for idx in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train reviews: [14, 69, 7, 8014, 2103, 17, 361, 3, 7278, 7111, 2745, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "test reviews: [148, 7, 98, 397, 12, 7, 8586, 31, 1897, 6463, 239, 12, 7920, 2, 1235, 3, 1761, 104, 4, 395, 2962, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "train sentiments: ['neg', 'neg', 'neg', 'neu', 'neg']\n",
      "test sentiments: ['neg', 'neg', 'neg', 'neu', 'neg']\n"
     ]
    }
   ],
   "source": [
    "print('train reviews:', train_rev[0])\n",
    "print('test reviews:', test_rev[0])\n",
    "print('\\ntrain sentiments:', train_sent[:5])\n",
    "print('test sentiments:', train_sent[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
